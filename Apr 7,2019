Apr 7,2019
//集成学习 Ensemble Learning
集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型
数据集大：划分成多个小数据集，学习多个模型进行组合
数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合

1. Bagging
Bagging是bootstrap aggregating的简写。先说一下bootstrap，bootstrap也称为自助法.
它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。
/具体步骤如下
1)采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本
2)根据抽出的样本计算想要得到的统计量T
3)重复上述N次（一般大于1000），得到N个统计量T
4)根据这N个统计量，即可计算出统计量的置信区间
在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型.
最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。

例如随机森林（Random Forest）就属于Bagging。
随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。

整合方式就是：分类问题用majority voting，回归用均值。

Boosting
提升方法（Boosting）是一种可以用来减小监督学习中偏差的机器学习算法。
主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是AdaBoost（Adaptive boosting）
算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，
也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。
